{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7815474,"sourceType":"datasetVersion","datasetId":4578415},{"sourceId":11370993,"sourceType":"datasetVersion","datasetId":7118423},{"sourceId":342697,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":286683,"modelId":307498}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport glob\nimport random\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim \nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torchvision.models import vgg19\n\n\n# Set the device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:43:13.701948Z","iopub.execute_input":"2025-04-17T16:43:13.702676Z","iopub.status.idle":"2025-04-17T16:43:13.708075Z","shell.execute_reply.started":"2025-04-17T16:43:13.702647Z","shell.execute_reply":"2025-04-17T16:43:13.707456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code]\nclass DIV2KDataset(Dataset):\n    def __init__(self, hr_dir, lr_dir, crop_size=96, scale=4, is_train=True):\n        \"\"\"\n        hr_dir: Directory containing high resolution images.\n        lr_dir: Directory containing low resolution images (e.g., bicubic downscaled images).\n        crop_size: Size of the HR crop (for training, a random crop will be taken).\n        scale: The downscaling factor (e.g., 4 for x4 super-resolution).\n        is_train: Flag indicating training or validation mode.\n        \"\"\"\n        self.hr_dir = hr_dir\n        self.lr_dir = lr_dir\n        self.hr_image_files = sorted(glob.glob(os.path.join(hr_dir, '*.png')) +\n                                     glob.glob(os.path.join(hr_dir, '*.jpg')))\n        self.lr_image_files = sorted(glob.glob(os.path.join(lr_dir, '*.png')) +\n                                     glob.glob(os.path.join(lr_dir, '*.jpg')))\n        self.crop_size = crop_size\n        self.scale = scale\n        self.is_train = is_train\n\n        # Define transforms for HR and LR images (initially using ToTensor only)\n        self.hr_transform = transforms.Compose([\n            transforms.ToTensor()\n        ])\n        self.lr_transform = transforms.Compose([\n            transforms.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.hr_image_files)\n\n    def __getitem__(self, idx):\n        # Load the HR and LR images\n        hr_image = Image.open(self.hr_image_files[idx]).convert(\"RGB\")\n        lr_image = Image.open(self.lr_image_files[idx]).convert(\"RGB\")\n        \n        # For training, perform a random crop on the HR image and derive the corresponding LR crop\n        if self.is_train:\n            # Random crop on HR\n            hr_width, hr_height = hr_image.size\n            if hr_width < self.crop_size or hr_height < self.crop_size:\n                hr_image = hr_image.resize((max(hr_width, self.crop_size), max(hr_height, self.crop_size)), Image.BICUBIC)\n                lr_image = lr_image.resize((max(lr_image.size), max(lr_image.size)), Image.BICUBIC)\n                hr_width, hr_height = hr_image.size\n\n            # Coordinates for crop\n            left = random.randint(0, hr_width - self.crop_size)\n            top = random.randint(0, hr_height - self.crop_size)\n            hr_crop = hr_image.crop((left, top, left + self.crop_size, top + self.crop_size))\n            \n            # Derive LR crop by resizing the HR crop to the LR size\n            lr_crop = hr_crop.resize((self.crop_size // self.scale, self.crop_size // self.scale), Image.BICUBIC)\n            \n            hr_image = hr_crop\n            lr_image = lr_crop\n        else:\n            # For validation: center crop if needed (or keep full image)\n            pass  # You can add center cropping here if desired\n\n        # Apply tensor transforms\n        hr_tensor = self.hr_transform(hr_image)\n        lr_tensor = self.lr_transform(lr_image)\n\n        return {\"lr\": lr_tensor, \"hr\": hr_tensor}\n\n# Example file paths on Kaggle:\n# Modify these paths according to your Kaggle dataset structure.\nTRAIN_HR_DIR = \"/kaggle/input/div2k-dataset-for-super-resolution/Dataset/DIV2K_train_HR\"\nTRAIN_LR_DIR = \"/kaggle/input/div2k-dataset-for-super-resolution/Dataset/DIV2K_train_LR_bicubic_X4/X4\"\n\n# Create a dataset instance and data loader\ntrain_dataset = DIV2KDataset(hr_dir=TRAIN_HR_DIR, lr_dir=TRAIN_LR_DIR, crop_size=96, scale=4, is_train=True)\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n\nprint(\"Number of training images:\", len(train_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:43:16.785877Z","iopub.execute_input":"2025-04-17T16:43:16.786149Z","iopub.status.idle":"2025-04-17T16:43:16.814357Z","shell.execute_reply.started":"2025-04-17T16:43:16.786130Z","shell.execute_reply":"2025-04-17T16:43:16.813659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code]\ndef show_sample_pair(dataset, idx=0):\n    sample = dataset[idx]\n    lr_img = transforms.ToPILImage()(sample[\"lr\"])\n    hr_img = transforms.ToPILImage()(sample[\"hr\"])\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n    axes[0].imshow(lr_img)\n    axes[0].set_title(\"LR Image\")\n    axes[0].axis(\"off\")\n    \n    axes[1].imshow(hr_img)\n    axes[1].set_title(\"HR Image\")\n    axes[1].axis(\"off\")\n    \n    plt.show()\n\n# Display a sample pair from the training set\nshow_sample_pair(train_dataset, idx=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:43:21.188843Z","iopub.execute_input":"2025-04-17T16:43:21.189396Z","iopub.status.idle":"2025-04-17T16:43:21.489833Z","shell.execute_reply.started":"2025-04-17T16:43:21.189373Z","shell.execute_reply":"2025-04-17T16:43:21.489006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generator model","metadata":{}},{"cell_type":"code","source":"# %% [code]\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(channels)\n        )\n    \n    def forward(self, x):\n        return x + self.block(x)\n\nclass Generator(nn.Module):\n    def __init__(self, num_residual_blocks=16, upscale_factor=4):\n        super(Generator, self).__init__()\n        # Initial convolution\n        self.initial = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Residual blocks\n        res_blocks = []\n        for _ in range(num_residual_blocks):\n            res_blocks.append(ResidualBlock(64))\n        self.res_blocks = nn.Sequential(*res_blocks)\n        \n        # Mid convolution layer after residual blocks\n        self.mid_conv = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64)\n        )\n        \n        # Upsampling layers using PixelShuffle – for upscale factor 4, use 2 steps of x2 upscaling\n        upsample_layers = []\n        num_upsamples = upscale_factor // 2\n        for _ in range(num_upsamples):\n            upsample_layers += [\n                nn.Conv2d(64, 256, kernel_size=3, padding=1),\n                nn.PixelShuffle(2),\n                nn.ReLU(inplace=True)\n            ]\n        self.upsample = nn.Sequential(*upsample_layers)\n        \n        # Final output convolution\n        self.final = nn.Conv2d(64, 3, kernel_size=9, padding=4)\n    \n    def forward(self, x):\n        initial_out = self.initial(x)\n        res_out = self.res_blocks(initial_out)\n        mid = self.mid_conv(res_out)\n        combined = initial_out + mid  # Skip connection\n        upsampled = self.upsample(combined)\n        output = self.final(upsampled)\n        return output\n\n# Instantiate the generator and move it to device:\nG = Generator(num_residual_blocks=8, upscale_factor=4).to(device)  # using 8 blocks for faster training\nprint(G)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:43:32.275113Z","iopub.execute_input":"2025-04-17T16:43:32.275413Z","iopub.status.idle":"2025-04-17T16:43:32.304844Z","shell.execute_reply.started":"2025-04-17T16:43:32.275390Z","shell.execute_reply":"2025-04-17T16:43:32.304186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generator model","metadata":{}},{"cell_type":"code","source":"# %% [code]\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        # A sequence of convolutional layers with increasing feature channels\n        self.net = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        # Fully connected layers for binary classification\n        self.fc = nn.Sequential(\n            nn.Linear(512 * 6 * 6, 1024),  # assuming input patch size of 96x96 leads to 6x6 feature maps\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(1024, 1)\n        )\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        features = self.net(x)\n        features = features.view(batch_size, -1)\n        out = self.fc(features)\n        return out\n\nD = Discriminator().to(device)\nprint(D)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:43:38.494279Z","iopub.execute_input":"2025-04-17T16:43:38.494868Z","iopub.status.idle":"2025-04-17T16:43:38.730824Z","shell.execute_reply.started":"2025-04-17T16:43:38.494843Z","shell.execute_reply":"2025-04-17T16:43:38.730008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.models import vgg19\nimport torch.nn.functional as F\n\nclass VGGFeatureExtractor(nn.Module):\n    def __init__(self, layer='features.35', use_cuda=True):\n        super().__init__()\n        vgg = vgg19(pretrained=True).eval()\n        self.features = nn.Sequential(*list(vgg.features.children())[:36])  # up through conv5_4\n        for p in self.features.parameters(): p.requires_grad = False\n        if use_cuda: self.features = self.features.to(device)\n\n    def forward(self, img):\n        # expects img in [0,1], normalize to VGG’s ImageNet stats\n        mean = torch.tensor([0.485, 0.456, 0.406], device=img.device).view(1,3,1,1)\n        std  = torch.tensor([0.229, 0.224, 0.225], device=img.device).view(1,3,1,1)\n        img_norm = (img - mean) / std\n        return self.features(img_norm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:44:04.415056Z","iopub.execute_input":"2025-04-17T16:44:04.415760Z","iopub.status.idle":"2025-04-17T16:44:04.421737Z","shell.execute_reply.started":"2025-04-17T16:44:04.415734Z","shell.execute_reply":"2025-04-17T16:44:04.420915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# Apply to G and D\nG.apply(weights_init)\nD.apply(weights_init)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:46:15.755896Z","iopub.execute_input":"2025-04-17T16:46:15.756475Z","iopub.status.idle":"2025-04-17T16:46:15.878788Z","shell.execute_reply.started":"2025-04-17T16:46:15.756451Z","shell.execute_reply":"2025-04-17T16:46:15.878187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pixel-wise (content) loss\npixel_loss_fn       = nn.MSELoss()\n\n# Adversarial loss\nadv_loss_fn         = nn.BCEWithLogitsLoss()\n\n# Loss weights\nlambda_pixel        = 1.0\nlambda_perceptual   = 0.01\nlambda_adv          = 0.001   # start smaller for stability","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:47:49.972964Z","iopub.execute_input":"2025-04-17T16:47:49.973245Z","iopub.status.idle":"2025-04-17T16:47:49.977474Z","shell.execute_reply.started":"2025-04-17T16:47:49.973225Z","shell.execute_reply":"2025-04-17T16:47:49.976797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer_G = optim.Adam(G.parameters(), lr=1e-4, betas=(0.9, 0.999))\noptimizer_D = optim.Adam(D.parameters(), lr=1e-4, betas=(0.9, 0.999))\n\n# Optional: decay LR every 50 epochs\nfrom torch.optim.lr_scheduler import StepLR\nscheduler_G = StepLR(optimizer_G, step_size=50, gamma=0.5)\nscheduler_D = StepLR(optimizer_D, step_size=50, gamma=0.5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:47:50.549837Z","iopub.execute_input":"2025-04-17T16:47:50.550117Z","iopub.status.idle":"2025-04-17T16:47:50.555479Z","shell.execute_reply.started":"2025-04-17T16:47:50.550095Z","shell.execute_reply":"2025-04-17T16:47:50.554770Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pretrain_epochs = 50\nfor epoch in range(pretrain_epochs):\n    G.train()\n    epoch_loss = 0\n    for batch in train_dataloader:\n        lr = batch['lr'].to(device)\n        hr = batch['hr'].to(device)\n        \n        optimizer_G.zero_grad()\n        sr = G(lr)\n        loss = pixel_loss_fn(sr, hr)\n        loss.backward()\n        optimizer_G.step()\n        \n        epoch_loss += loss.item()\n    print(f\"[Pretrain {epoch+1}/{pretrain_epochs}] MSE loss: {epoch_loss/len(train_dataloader):.4f}\")\n\n# Save the pretrained weights\ntorch.save(G.state_dict(), \"generator_pretrained.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:47:52.949972Z","iopub.execute_input":"2025-04-17T16:47:52.950206Z","iopub.status.idle":"2025-04-17T17:24:50.592210Z","shell.execute_reply.started":"2025-04-17T16:47:52.950190Z","shell.execute_reply":"2025-04-17T17:24:50.591397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"adv_epochs = 200\ncontent_extractor = VGGFeatureExtractor(use_cuda=torch.cuda.is_available())\n\nfor epoch in range(adv_epochs):\n    G.train(); D.train()\n    d_loss_running = 0\n    g_loss_running = 0\n    \n    for batch in train_dataloader:\n        lr = batch['lr'].to(device)\n        hr = batch['hr'].to(device)\n        bs = lr.size(0)\n        \n        # ——— Train Discriminator ———\n        D.zero_grad()\n        \n        # Real\n        real_out = D(hr)\n        real_labels = torch.ones(bs, 1, device=device)\n        loss_real = adv_loss_fn(real_out, real_labels)\n        \n        # Fake (from frozen G)\n        with torch.no_grad():\n            fake_hr = G(lr)\n        fake_out = D(fake_hr)\n        fake_labels = torch.zeros(bs, 1, device=device)\n        loss_fake = adv_loss_fn(fake_out, fake_labels)\n        \n        d_loss = 0.5 * (loss_real + loss_fake)\n        d_loss.backward()\n        optimizer_D.step()\n        \n        # ——— Train Generator ———\n        G.zero_grad()\n        \n        fake_hr = G(lr)\n        pred_fake = D(fake_hr)\n        adv_loss = adv_loss_fn(pred_fake, real_labels)\n        \n        perc_loss = F.l1_loss(\n            content_extractor(fake_hr),\n            content_extractor(hr)\n        )\n        pix_loss = pixel_loss_fn(fake_hr, hr)\n        \n        g_loss = (lambda_adv * adv_loss\n                  + lambda_perceptual * perc_loss\n                  + lambda_pixel * pix_loss)\n        g_loss.backward()\n        optimizer_G.step()\n        \n        d_loss_running += d_loss.item()\n        g_loss_running += g_loss.item()\n    \n    # Step schedulers\n    scheduler_G.step()\n    scheduler_D.step()\n    \n    print(f\"Epoch {epoch+1}/{adv_epochs} — D_loss: {d_loss_running/len(train_dataloader):.4f}, \"\n          f\"G_loss: {g_loss_running/len(train_dataloader):.4f}\")\n    \n    # Save checkpoints every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        torch.save(G.state_dict(), f\"generator_epoch{epoch+1}.pth\")\n        torch.save(D.state_dict(), f\"discriminator_epoch{epoch+1}.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:26:32.225743Z","iopub.execute_input":"2025-04-17T17:26:32.226085Z","iopub.status.idle":"2025-04-17T19:54:55.315288Z","shell.execute_reply.started":"2025-04-17T17:26:32.226059Z","shell.execute_reply":"2025-04-17T19:54:55.314263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport torch\nimport shutil\nfrom skimage.metrics import peak_signal_noise_ratio\n\nbest_psnr = 0.0\nbest_ckpt = None\n\n# Loop through all saved generator checkpoints\nfor ckpt_path in sorted(glob.glob(\"generator_epoch*.pth\")):\n    # Load weights\n    G.load_state_dict(torch.load(ckpt_path, map_location=device))\n    G.to(device).eval()\n    \n    # Compute PSNR over the entire validation set\n    psnrs = []\n    with torch.no_grad():\n        for batch in valid_loader:\n            lr = batch['lr'].to(device)\n            hr = batch['hr'].to(device)\n            \n            # Forward pass\n            sr = G(lr).clamp(0,1).cpu()\n            hr_cpu = hr.cpu()\n            \n            # Convert to H×W×C numpy arrays\n            sr_np = sr.squeeze(0).permute(1,2,0).numpy()\n            hr_np = hr_cpu.squeeze(0).permute(1,2,0).numpy()\n            \n            # Measure PSNR\n            psnrs.append(peak_signal_noise_ratio(hr_np, sr_np, data_range=1.0))\n    \n    avg_psnr = sum(psnrs) / len(psnrs)\n    print(f\"{ckpt_path:20s} → PSNR = {avg_psnr:.2f} dB\")\n    \n    if avg_psnr > best_psnr:\n        best_psnr = avg_psnr\n        best_ckpt  = ckpt_path\n\nprint(f\"\\n🏆 Best checkpoint: {best_ckpt} with PSNR = {best_psnr:.2f} dB\")\n\n# Copy the best model to a stable filename\nshutil.copy(best_ckpt, \"generator_best.pth\")\nprint(\"✅ Copied to generator_best.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:04:05.658452Z","iopub.execute_input":"2025-04-17T20:04:05.659219Z","iopub.status.idle":"2025-04-17T20:21:49.725724Z","shell.execute_reply.started":"2025-04-17T20:04:05.659192Z","shell.execute_reply":"2025-04-17T20:21:49.724819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from skimage.metrics import peak_signal_noise_ratio\nimport torch.nn.functional as F\n\nbic_psnrs = []\nwith torch.no_grad():\n    for batch in valid_loader:\n        lr = batch['lr']    # [1,3,h,w] ∈ [0,1]\n        hr = batch['hr']    # [1,3,H,W] ∈ [0,1]\n        # Bicubic upsample\n        bic = F.interpolate(lr, size=hr.shape[-2:], mode='bicubic', align_corners=False)\n        # Convert to numpy\n        bic_np = bic.squeeze(0).permute(1,2,0).cpu().numpy()\n        hr_np  = hr.squeeze(0).permute(1,2,0).cpu().numpy()\n        bic_psnrs.append(peak_signal_noise_ratio(hr_np, bic_np, data_range=1.0))\n\nprint(f\"🔹 Bicubic baseline PSNR: {sum(bic_psnrs)/len(bic_psnrs):.2f} dB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:23:05.396151Z","iopub.execute_input":"2025-04-17T20:23:05.396545Z","iopub.status.idle":"2025-04-17T20:23:17.601165Z","shell.execute_reply.started":"2025-04-17T20:23:05.396517Z","shell.execute_reply":"2025-04-17T20:23:17.600258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom skimage.filters import sobel\n\n# 1) Load model on CPU\ndevice = torch.device('cpu')\nG = Generator(num_residual_blocks=8, upscale_factor=4)\nG.load_state_dict(torch.load(\"generator_best.pth\", map_location=device))\nG.to(device).eval()\n\n# 2) Grab one sample\nidx = 1\nsample = test_ds[idx]\nlr  = sample['lr'].unsqueeze(0).to(device)   # [1,3,h,w]\nhr  = sample['hr'].unsqueeze(0).to(device)   # [1,3,H,W]\n\n# 3) Inference + bicubic baseline (both at HR size)\nwith torch.no_grad():\n    sr  = G(lr).clamp(0,1)\nbic = F.interpolate(lr, size=hr.shape[-2:], mode='bicubic', align_corners=False)\n\n# 4) Convert to H×W×C numpy arrays\ndef to_np(x):\n    return x.squeeze(0).permute(1,2,0).cpu().numpy()\n\nbic_np, sr_np, hr_np = map(to_np, (bic, sr, hr))\n\n# 5) Center‑crop up to 64×64\nH, W, _ = hr_np.shape\npatch = min(64, H, W)\nhalf  = patch//2\nch, cw = H//2, W//2\ny0, y1 = ch-half, ch+half\nx0, x1 = cw-half, cw+half\n\nbic_c = bic_np[y0:y1, x0:x1]\nsr_c  = sr_np[y0:y1,  x0:x1]\nhr_c  = hr_np[y0:y1,  x0:x1]\n\n# 6) Difference heatmap\ndiff = np.abs(sr_c - bic_c).mean(axis=2)\n\n# 7) Edge maps\nedges_sr  = sobel(sr_c.mean(axis=2))\nedges_bic = sobel(bic_c.mean(axis=2))\n\n# 8) Plot 2×3 grid\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\n\naxes[0,0].imshow(bic_c, interpolation='nearest')\naxes[0,0].set_title(\"Bicubic Crop\")\naxes[0,0].axis('off')\n\naxes[0,1].imshow(sr_c, interpolation='nearest')\naxes[0,1].set_title(\"SRGAN Crop\")\naxes[0,1].axis('off')\n\naxes[0,2].imshow(hr_c, interpolation='nearest')\naxes[0,2].set_title(\"Ground‑Truth HR\")\naxes[0,2].axis('off')\n\naxes[1,0].imshow(diff, cmap='magma')\naxes[1,0].set_title(\"|SR – Bicubic|\")\naxes[1,0].axis('off')\n\naxes[1,1].imshow(sr_c, interpolation='nearest')\naxes[1,1].contour(edges_sr,  colors='r', linewidths=1)\naxes[1,1].set_title(\"SRGAN Edges (red)\")\naxes[1,1].axis('off')\n\naxes[1,2].imshow(bic_c, interpolation='nearest')\naxes[1,2].contour(edges_bic, colors='b', linewidths=1)\naxes[1,2].set_title(\"Bicubic Edges (blue)\")\naxes[1,2].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:31:20.762213Z","iopub.execute_input":"2025-04-17T20:31:20.762922Z","iopub.status.idle":"2025-04-17T20:31:40.074028Z","shell.execute_reply.started":"2025-04-17T20:31:20.762901Z","shell.execute_reply":"2025-04-17T20:31:40.073312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}